{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for Training\n",
    "\n",
    "### After cleaning the data, the next step is to package it for training. In this section, we will learn how to format the training data for use with Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before training, additional data preparation is required even after the cleaning process. \n",
    "\n",
    "### Key steps include:\n",
    "1. **Tokenization**: Breaking text into smaller meaningful units called tokens. By meaningful we meant something that makes meaning to LLM. LLM does not understand text but numbers. Tokenization is a technique to transform these human readable texts to represent as numbers\n",
    "2. **Packing**: Organizing tokens into a fixed maximum sequence length to enhance training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385736a250294135ba00d6167da9fcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'meta'],\n",
      "    num_rows: 40473\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load the dataset we just stored\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"parquet\", \n",
    "    data_files=\"./data/preprocessed_dataset.parquet\", \n",
    "    split=\"train\"\n",
    ")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'meta'],\n",
      "    num_rows: 4048\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# since we do not enough memory to process the entire dataset, we will\n",
    "# use the shard method from the Hugging Face Dataset object to split\n",
    "# the dataset into 10 smaller pieces, or shards\n",
    "dataset = dataset.shard(num_shards=10, index=0)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# we will load the tokenizer\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"facebook/opt-125m\")\n",
    "tokenizer = pipe.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " '!',\n",
       " 'ĠI',\n",
       " 'Ġlove',\n",
       " 'Ġbuilding',\n",
       " 'Ġapplication',\n",
       " 'Ġwith',\n",
       " 'ĠGener',\n",
       " 'ative',\n",
       " 'ĠAI',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the tokenized text looks like\n",
    "tokenizer.tokenize(\"Hi! I love building application with Generative AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 30086, 328, 38, 657, 745, 2502, 19, 15745, 3693, 4687, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following are the token ids generated by the tokenizer\n",
    "tokenizer.encode(\"Hi! I love building application with Generative AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following helper function will be used to tokenize all the examples\n",
    "def tokenization(example):\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(example[\"text\"])\n",
    "\n",
    "    # Convert tokens to ids\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Add <bos>, <eos> tokens to the front and back of tokens_ids \n",
    "    # bos: begin of sequence, eos: end of sequence\n",
    "    token_ids = [\n",
    "        tokenizer.bos_token_id] \\\n",
    "        + token_ids \\\n",
    "        + [tokenizer.eos_token_id\n",
    "    ]\n",
    "    example[\"input_ids\"] = token_ids\n",
    "\n",
    "    # We will be using this column to count the total number of tokens \n",
    "    # in the final dataset\n",
    "    example[\"num_tokens\"] = len(token_ids)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1924c2e693c44fc884dd72c94a4173e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'meta', 'input_ids', 'num_tokens'],\n",
      "    num_rows: 4048\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# lets tokenize all the examples in the pretraining dataset\n",
    "dataset = dataset.map(tokenization, load_from_cache_file=False)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: The Colorado Climate Center pr\n",
      "\n",
      "input_ids: [2, 133, 3004, 11001, 824, 1639, 8979, 8, 414, 15, 7635, 6, 7749, 6, 24400, 810, 6, 9007, 24263, 6, 12530, 6, 8, 70, 97, 2147, 5129, 2476, 4, 50118]\n",
      "\n",
      "num_tokens: 488\n"
     ]
    }
   ],
   "source": [
    "# we can see we have created new columns like input_ids and num_tokens\n",
    "# lets take and example and see how it looks like\n",
    "sample = dataset[3]\n",
    "\n",
    "print(\"text:\", sample[\"text\"][:30]) # \n",
    "print(\"\\ninput_ids:\", sample[\"input_ids\"][:30])\n",
    "print(\"\\nnum_tokens:\", sample[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4624174"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the total number of tokens in the dataset\n",
    "# you can see that with just few data, the token reached to around 4.5 millions\n",
    "# LLMs are trained with huge amount of text data which easily amoubts to billions of tokens\n",
    "import numpy as np\n",
    "np.sum(dataset[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4624174\n"
     ]
    }
   ],
   "source": [
    "# now, we will be packing the data\n",
    "# let's concatenate input_ids for all examples into a single list\n",
    "input_ids = np.concatenate(dataset[\"input_ids\"])\n",
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4624160\n"
     ]
    }
   ],
   "source": [
    "# lets take the maximum sequence length of packing as 32\n",
    "# total_length will be the number of tokens in the dataset with 32 as maximum sequence length\n",
    "max_seq_length = 32\n",
    "total_length = len(input_ids) - len(input_ids) % max_seq_length\n",
    "print(total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4624160,)\n"
     ]
    }
   ],
   "source": [
    "# Discard extra tokens from end of the list so number of tokens is exactly divisible by max_seq_length\n",
    "input_ids = input_ids[:total_length]\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144505, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now create a new array of shape (num_examples, max_seq_length)\n",
    "# and fill it with the input_ids\n",
    "input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)\n",
    "input_ids_reshaped.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_ids_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 144505\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# now we will convert dataset into Hugging Face Dataset object\n",
    "input_ids_list = input_ids_reshaped.tolist()\n",
    "packaged_pretrain_dataset = datasets.Dataset.from_dict(\n",
    "    {\"input_ids\": input_ids_list}\n",
    ")\n",
    "print(packaged_pretrain_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dee26f330f46d58a2881d79c46d6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/145 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19074660"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the packed dataset to disk\n",
    "packaged_pretrain_dataset.to_parquet(\"./data/packaged_pretrain_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
