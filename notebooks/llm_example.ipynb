{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Pretrained LLMs for Text Generation\n",
    "\n",
    "### Pretrained large language models (LLMs) have transformed natural language processing by providing developers with access to vast knowledge encoded within these models. These models, trained on extensive datasets, excel at tasks like text generation, summarization, translation, and more. By leveraging an existing model, you can explore its capabilities and adapt it for specific applications without the need to train from scratch. This not only saves time and computational resources but also enables you to achieve cutting-edge results efficiently.\n",
    "\n",
    "### In this repository, we will focus on the pretraining phase, which is the foundational step in building any LLM from scratch. Fine-tuning is beyond the scope of this exploration. Dive in, experiment, and enjoy the journey of understanding how these powerful models are built!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecations)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define seed for reporoducibility\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Initialize a text-generation pipeline using the OPT-125M model from Facebook.\n",
    "# The OPT-125M model is a smaller version of the OPT (Open Pretrained Transformer) family, \n",
    "# designed for efficient text generation tasks while maintaining reasonable performance.\n",
    "pipe = pipeline(\"text-generation\", model=\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs are awesome. I love them.\n",
      "I love them too. I'm a big fan of the ones that are made by the same people.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"LLMs are awesome. I love\"\n",
    "\n",
    "# Generate text\n",
    "output = pipe(prompt, max_length=128, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def fibonacci(n):\n",
      "\n",
      "def\n"
     ]
    }
   ],
   "source": [
    "# lets check if this specific LLM is able to generate code\n",
    "prompt = \"def fibonacci(n):\"\n",
    "\n",
    "# Generate code\n",
    "output = pipe(prompt, max_length=128, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Code Generation\n",
    "\n",
    "The text generation model used in this notebook, `facebook/opt-125m`, appears to lack sufficient training on code-specific datasets. This is evident from its inability to generate coherent and relevant code when prompted with a code-related input, such as `def fibonacci(n):`. The generated output contains repetitive and nonsensical text, indicating that the model is not well-suited for tasks involving code generation. \n",
    "\n",
    "For better results in code generation tasks, it is recommended to use models specifically trained on code data, such as OpenAI's Codex or Hugging Face's CodeGen models. These models are fine-tuned on programming-related datasets and are better equipped to handle such tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
