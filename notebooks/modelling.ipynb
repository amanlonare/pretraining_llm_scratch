{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing model for training\n",
    "\n",
    "### In this notebook, we are going to configure models based on Meta's Llama family of models. The transformers library has several tools for working with these models\n",
    "\n",
    "### Start by creating a **LlamaConfig** object to configure the architecture of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecation warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LlamaConfig is a configuration class for the Llama model\n",
    "from transformers import LlamaConfig\n",
    "config = LlamaConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now lets try to update parameters to change the model architecture\n",
    "config.num_hidden_layers = 12      # reduced from 32 to 12\n",
    "config.hidden_size = 1024          # reduced 1/4 from 4096 to 1024\n",
    "config.intermediate_size = 4096    # reduced 1/3 from 11008 to 4096 (dimension of MLP representations)\n",
    "config.num_key_value_heads = 8     # reduced 1/4 from 32 to 8 (defaults to num_attention_heads=32)\n",
    "config.torch_dtype = \"bfloat16\"    # for half-precision training\n",
    "config.use_cache = False           # `True` is incompatible w/ gradient checkpointing\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will explore four different ways to initialize the weights of a model for training:\n",
    "\n",
    "1. **Random weight initialization**\n",
    "2. **Using an existing model for continued pre-training**\n",
    "1. **Downscaling an existing model**\n",
    "1. **Upscaling an existing model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1. Randonly initialize the weights of the model\n",
    "from transformers import LlamaForCausalLM\n",
    "model = LlamaForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 342385664\n"
     ]
    }
   ],
   "source": [
    "# print the number of parameters in the model defined\n",
    "def print_nparams(model):\n",
    "    \"\"\"Calculate the total number of model parameters\"\"\"\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"The total number of parameters is: {nparams}\")\n",
    "\n",
    "print_nparams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 weights of layer 'model.layers.0.self_attn.q_proj.weight':\n",
      "tensor([ 0.0217,  0.0204, -0.0008,  0.0087, -0.0089, -0.0291,  0.0166, -0.0086,\n",
      "         0.0004,  0.0017, -0.0089, -0.0095, -0.0135, -0.0160, -0.0148, -0.0131,\n",
      "         0.0104,  0.0200,  0.0348,  0.0110,  0.0082, -0.0011, -0.0233, -0.0113,\n",
      "         0.0087,  0.0267, -0.0030, -0.0272, -0.0098, -0.0089])\n"
     ]
    }
   ],
   "source": [
    "# Take a look at a sample of the weights in a single layer\n",
    "layer_name = \"model.layers.0.self_attn.q_proj.weight\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name == layer_name:\n",
    "        print(f\"First 30 weights of layer '{layer_name}':\")\n",
    "        print(param.data.view(-1)[:30])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " artifacts enough enough enough enough enough enough enough enough enough enough enough enough enough enough enough enough enough enough humanitarian enough enough humanitarian enough humanitarian humanitarian humanitarian humanitarian humanitarian humanitarian humanitarian humanitarian humanitarian hyper sponsors sponsorsseason c hyperseason c c c c c c upheaval upheaval upheaval upheaval upheaval upheaval upheaval upheaval ne nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut nut\n"
     ]
    }
   ],
   "source": [
    "# lets load the tokenizer\n",
    "# we will try using the model for inference. \n",
    "# Note that the current model has randomly initialized weights\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"facebook/opt-125m\")\n",
    "\n",
    "# Load a tokenizer from the facebook/opt-125m, \n",
    "# which is compatible with the Llama-2 tokenizer\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "# Run simple inference with prompt\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"I am a human. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# we will see random outputs because 'model' is randomly initialized and not trained yet\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the model from memory to avoid crashing the kerne\n",
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "import gc\n",
    "del model\n",
    "del streamer\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# 2. Reuse general pretrained model weights\n",
    "# we will load an existing model and can use it as it is to continue pretraining on new data\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"facebook/opt-125m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = pipe.model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the model from memory to avoid crashing the kernel\n",
    "# NOTE: We're running large models in a limited environment. Run me if you encounter any memory issues.\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# 3. Downscaling from a general pretrained model\n",
    "# we will downscale the model to a smaller size so we will\n",
    "# downscale our facebook/opt-125m model from a 12 layer model to a 10 layer model\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"facebook/opt-125m\")\n",
    "tokenizer = pipe.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pipe.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 125239296\n"
     ]
    }
   ],
   "source": [
    "model = pipe.model\n",
    "print_nparams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers after removal: 10\n"
     ]
    }
   ],
   "source": [
    "# Remove the middle two layers (layers 5 and 6) and update the configuration\n",
    "decoder = model.model.decoder  # OPTDecoder\n",
    "layers = decoder.layers  # ModuleList\n",
    "\n",
    "# Current layers: 0,1,2,3,4,5,6,7,8,9,10,11\n",
    "# We want to remove layers 5,6 (the middle ones)\n",
    "middle_start = len(layers) // 2 - 1  # 5\n",
    "new_layers = torch.nn.ModuleList(\n",
    "    list(layers[:middle_start]) +  # layers 0-4\n",
    "    list(layers[middle_start + 2:])  # layers 7-11\n",
    ")\n",
    "\n",
    "# Replace the layers\n",
    "decoder.layers = new_layers\n",
    "\n",
    "# Verify the number of layers\n",
    "print(f\"Number of layers after removal: {len(decoder.layers)}\")  # Should show 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear the memory to avoid crashing the kernel\n",
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTConfig {\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 3072,\n",
      "  \"hidden_size\": 768,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 768\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Depth Upscaling from a general pretrained model\n",
    "# It is a technique to upscale a model by adding more layers to it.\n",
    "# We will upscale our facebook/opt-125m model from a 12 layer model to a 16 layer model\n",
    "# The following are the steps we will take\n",
    "#   1. Configure a 16 layer model and initialize it with random weights\n",
    "#   2. Load the 12 layer facebook/opt-125m model into memory\n",
    "#   3. Copy the bottom 8 and top 8 layers from the 12 layer model and use them to overwrite the random weights of the 16 layer model\n",
    "#   4. Copy over the embedding and classifying layers to replace the randomly initialized counterparts in the 16 layer model\n",
    "from transformers import OPTConfig\n",
    "\n",
    "# For the OPT model, the layers are organized differently than LLaMA\n",
    "# 1. Configure a 16 layer model\n",
    "# config = OPTConfig(\n",
    "#     num_hidden_layers=16,  # We want our model to have 16 final layers\n",
    "#     hidden_size=1024,  \n",
    "#     intermediate_size=4096,  \n",
    "#     num_attention_heads=32,  \n",
    "#     num_key_value_heads=8,\n",
    "#     torch_dtype=\"bfloat16\",\n",
    "#     use_cache=False \n",
    "# )\n",
    "config = OPTConfig(\n",
    "    num_hidden_layers=16,        # Increase from 12 to 16 layers\n",
    "    hidden_size=768,            # Same as OPT-125m\n",
    "    ffn_dim=3072,              # Same as OPT-125m\n",
    "    num_attention_heads=12,     # Same as OPT-125m\n",
    "    vocab_size=50272,          # Same as OPT-125m\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    use_cache=False\n",
    ")\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 153590784\n"
     ]
    }
   ],
   "source": [
    "# the number of parameters are around 221M of the upscaled model\n",
    "model = OPTForCausalLM(config)\n",
    "model = model.to(dtype=torch.bfloat16)  # convert to bfloat16\n",
    "print_nparams(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters is: 125239296\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"facebook/opt-125m\")\n",
    "\n",
    "pretrained_model = pipe.model\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "# the number of parameters are around 125M of the pretrained exisiting model\n",
    "print_nparams(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers after upscaling: 16\n",
      "The total number of parameters is: 192199680\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Take first 8 and last 8 layers from pretrained model's 12 layers\n",
    "model.model.decoder.layers = deepcopy(pretrained_model.model.decoder.layers[:8]) \\\n",
    "    + deepcopy(pretrained_model.model.decoder.layers[-8:])\n",
    "\n",
    "# 4. Copy embeddings and other components\n",
    "model.model.decoder.embed_tokens = deepcopy(pretrained_model.model.decoder.embed_tokens)\n",
    "model.lm_head = deepcopy(pretrained_model.lm_head)\n",
    "\n",
    "print(f\"Number of layers after upscaling: {len(model.model.decoder.layers)}\")  # Should show 16\n",
    "print_nparams(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 3072,\n",
      "  \"hidden_size\": 768,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 768\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the game and I am a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game. I am not a fan of the game.\n"
     ]
    }
   ],
   "source": [
    "# Run simple inference to show no trained model\n",
    "prompt = \"I am a GenAI expert. I love\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
